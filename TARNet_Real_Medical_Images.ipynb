{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define TARNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(128, 256) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn_feat = CNNFeatureExtractor()\n",
    "\n",
    "class TARNet(nn.Module):\n",
    "    def __init__(self, cnn_feat, rep_dim=256, hidden_dim=200):\n",
    "        super(TARNet, self).__init__()\n",
    "        self.cnn_feat = cnn_feat\n",
    "        \n",
    "        self.head_0 = nn.Sequential(\n",
    "            nn.Linear(rep_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.head_1 = nn.Sequential(\n",
    "            nn.Linear(rep_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t=None):\n",
    "        rep = self.cnn_feat(x)\n",
    "        y0 = self.head_0(rep).squeeze(-1)\n",
    "        y1 = self.head_1(rep).squeeze(-1)\n",
    "        \n",
    "        if t is None:\n",
    "            return y0, y1, rep\n",
    "        else:\n",
    "            y = torch.where(t == 1, y1, y0)\n",
    "            return y, y0, y1, rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-trained model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "loaded_obj = torch.load('tarnet_arch.pth', map_location=device, weights_only=False)\n",
    "\n",
    "model = TARNet(cnn_feat).to(device)\n",
    "model.load_state_dict(loaded_obj)\n",
    "\n",
    "model.eval()\n",
    "print(\"Loaded pre-trained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset: Real Images + Synthetic Confounders U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Load real medical images from Data/train or Data/test\n",
    "    Generate only missing confounders U and treatment assignments\n",
    "    Use actual disease category to define outcomes\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, split='train', image_size=64, \n",
    "                 confounder_dim=8, beta_strength=1.0):\n",
    "        self.data_dir = Path(data_dir) / split\n",
    "        self.image_size = image_size\n",
    "        self.confounder_dim = confounder_dim\n",
    "        self.beta_strength = beta_strength\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.categories = []\n",
    "        \n",
    "        for category in ['NORMAL', 'PNEUMONIA', 'COVID19']:\n",
    "            category_path = self.data_dir / category\n",
    "            if category_path.exists():\n",
    "                for img_path in sorted(category_path.glob('*.jpg')):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.categories.append(category)\n",
    "        \n",
    "        print(f\"\\nLoaded {len(self.image_paths)} images from {split} split\")\n",
    "        category_counts = pd.Series(self.categories).value_counts()\n",
    "        print(f\"Categories: {category_counts.to_dict()}\")\n",
    "        \n",
    "        self._generate_confounders_and_treatment()\n",
    "    \n",
    "    def _load_image(self, img_path):\n",
    "        \"\"\"Load and preprocess a single image\"\"\"\n",
    "        img = Image.open(img_path).convert('L')  # Grayscale\n",
    "        img = img.resize((self.image_size, self.image_size))\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = (img - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "        return torch.FloatTensor(img).unsqueeze(0)\n",
    "    \n",
    "    def _generate_confounders_and_treatment(self):\n",
    "        \"\"\"\n",
    "        Generate missing confounders U and treatment assignments.\n",
    "        U represents unmeasured patient characteristics (comorbidities, genetics, etc.)\n",
    "        Treatment assignment depends on both observed images and unobserved U.\n",
    "        \"\"\"\n",
    "        n = len(self.image_paths)\n",
    "        \n",
    "        Z2 = np.random.randn(n, self.confounder_dim)\n",
    "        \n",
    "        self.U = Z2 + np.random.randn(n, self.confounder_dim) * 0.1\n",
    "        \n",
    "        # Map disease categories to severity scores (for treatment propensity)?????\n",
    "        severity_map = {'NORMAL': 0.0, 'PNEUMONIA': 0.5, 'COVID19': 1.0}\n",
    "        severity = np.array([severity_map[cat] for cat in self.categories])\n",
    "        \n",
    "        print(\"Generate treatment assignments:\")\n",
    "        # Treatment propensity based on:\n",
    "        # - Disease severity (observable from images)\n",
    "        # - Confounders U (unobservable patient characteristics)\n",
    "        alpha = 2.0  # Weight for disease severity\n",
    "        beta = np.random.randn(self.confounder_dim)\n",
    "        beta = beta / np.linalg.norm(beta) * self.beta_strength\n",
    "        \n",
    "        logits = alpha * severity + self.U @ beta\n",
    "        propensity = 1 / (1 + np.exp(-logits))\n",
    "        self.T = (np.random.rand(n) < propensity).astype(np.int64)\n",
    "        \n",
    "        print(f\"Treatment rate: {self.T.mean():.3f}\")\n",
    "        print(f\"Treatment by category:\")\n",
    "        for cat in ['NORMAL', 'PNEUMONIA', 'COVID19']:\n",
    "            mask = np.array(self.categories) == cat\n",
    "            if mask.sum() > 0:\n",
    "                print(f\"  {cat}: {self.T[mask].mean():.3f}\")\n",
    "        \n",
    "        base_outcome = severity * 5.0 + self.U[:, 0] * 1.0 + np.random.randn(n) * 0.5\n",
    "        \n",
    "        # Treatment effect: more effective for severe cases\n",
    "        treatment_effect = -(2.0 + severity * 3.0 + self.U[:, 1] * 0.5)\n",
    "        \n",
    "        # Potential outcomes\n",
    "        self.Y0 = base_outcome + np.random.randn(n) * 0.3  # Without treatment\n",
    "        self.Y1 = base_outcome + treatment_effect + np.random.randn(n) * 0.3  # With treatment\n",
    "        \n",
    "        # Observed outcome (factual)\n",
    "        self.Y = self.T * self.Y1 + (1 - self.T) * self.Y0\n",
    "        \n",
    "        # True individual treatment effect\n",
    "        self.true_ITE = self.Y1 - self.Y0\n",
    "        \n",
    "        print(f\"Outcome statistics:\")\n",
    "        print(f\"  Y0 (control): mean={self.Y0.mean():.3f}, std={self.Y0.std():.3f}\")\n",
    "        print(f\"  Y1 (treated): mean={self.Y1.mean():.3f}, std={self.Y1.std():.3f}\")\n",
    "        print(f\"  True ATE: {self.true_ITE.mean():.3f}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self._load_image(self.image_paths[idx])\n",
    "        return {\n",
    "            'image': img,\n",
    "            'treatment': torch.LongTensor([self.T[idx]])[0],\n",
    "            'outcome': torch.FloatTensor([self.Y[idx]])[0],\n",
    "            'y0': torch.FloatTensor([self.Y0[idx]])[0],\n",
    "            'y1': torch.FloatTensor([self.Y1[idx]])[0],\n",
    "            'ite': torch.FloatTensor([self.true_ITE[idx]])[0],\n",
    "            'category': self.categories[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Real Medical Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 5144 images from train split\n",
      "Categories: {'PNEUMONIA': 3418, 'NORMAL': 1266, 'COVID19': 460}\n",
      "Generate treatment assignments:\n",
      "Treatment rate: 0.668\n",
      "Treatment by category:\n",
      "  NORMAL: 0.506\n",
      "  PNEUMONIA: 0.702\n",
      "  COVID19: 0.859\n",
      "Outcome statistics:\n",
      "  Y0 (control): mean=2.099, std=1.809\n",
      "  Y1 (treated): mean=-1.166, std=1.389\n",
      "  True ATE: -3.265\n",
      "\n",
      "Loaded 1288 images from test split\n",
      "Categories: {'PNEUMONIA': 855, 'NORMAL': 317, 'COVID19': 116}\n",
      "Generate treatment assignments:\n",
      "Treatment rate: 0.672\n",
      "Treatment by category:\n",
      "  NORMAL: 0.470\n",
      "  PNEUMONIA: 0.719\n",
      "  COVID19: 0.879\n",
      "Outcome statistics:\n",
      "  Y0 (control): mean=2.158, std=1.851\n",
      "  Y1 (treated): mean=-1.108, std=1.369\n",
      "  True ATE: -3.266\n",
      "\n",
      "DataLoaders created with batch_size=32\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'Data'\n",
    "\n",
    "image_size = 64\n",
    "confounder_dim = 8  # Dimension of unobserved confounders U\n",
    "beta_strength = 1.0  # Strength of confounding (higher = more bias)\n",
    "\n",
    "train_dataset = MedicalImageDataset(\n",
    "    data_dir=data_dir,\n",
    "    split='train',\n",
    "    image_size=image_size,\n",
    "    confounder_dim=confounder_dim,\n",
    "    beta_strength=beta_strength\n",
    ")\n",
    "\n",
    "test_dataset = MedicalImageDataset(\n",
    "    data_dir=data_dir,\n",
    "    split='test',\n",
    "    image_size=image_size,\n",
    "    confounder_dim=confounder_dim,\n",
    "    beta_strength=beta_strength\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch_size={batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning TARNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training TARNet:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_losses\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining TARNet:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarnet_medical_images.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel saved to tarnet_medical_images.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     11\u001b[0m         images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m         treatments \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreatment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[15], line 97\u001b[0m, in \u001b[0;36mMedicalImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 97\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: img,\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreatment\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mLongTensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT[idx]])[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories[idx]\n\u001b[1;32m    106\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[15], line 34\u001b[0m, in \u001b[0;36mMedicalImageDataset._load_image\u001b[0;34m(self, img_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Grayscale\u001b[39;00m\n\u001b[1;32m     33\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size))\n\u001b[0;32m---> 34\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     35\u001b[0m img \u001b[38;5;241m=\u001b[39m (img \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Normalize to [-1, 1]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/PIL/Image.py:724\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/PIL/Image.py:785\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    783\u001b[0m     encoder_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m--> 785\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/PIL/Image.py:873\u001b[0m, in \u001b[0;36mImage.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    870\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot decode image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core\u001b[38;5;241m.\u001b[39mPixelAccess \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;124;03m    Allocates storage for the image and loads the pixel data.  In\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;124;03m    normal cases, you don't need to call this method, since the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :rtype: :py:class:`.PixelAccess`\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mdirty:\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;66;03m# realize palette\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, num_epochs=30, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            treatments = batch['treatment'].to(device).float()\n",
    "            outcomes = batch['outcome'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred, _, _, _ = model(images, treatments)\n",
    "            loss = criterion(y_pred, outcomes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "print(\"\\nTraining TARNet:\")\n",
    "train_losses = train_model(model, train_loader, num_epochs=30, lr=0.001)\n",
    "\n",
    "torch.save(model.state_dict(), 'tarnet_medical_images.pth')\n",
    "print(\"\\nModel saved to tarnet_medical_images.pth\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "√PEHE (lower is better):    0.9161\n",
      "True ATE:                   -3.2658\n",
      "Predicted ATE:              -3.0253\n",
      "ATE Error:                  0.2406\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluate TARNet and compute PEHE and ATE errors\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_y0_pred = []\n",
    "    all_y1_pred = []\n",
    "    all_y0_true = []\n",
    "    all_y1_true = []\n",
    "    all_ite_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            y0_pred, y1_pred, _ = model(images, t=None)\n",
    "            \n",
    "            all_y0_pred.append(y0_pred.cpu().numpy())\n",
    "            all_y1_pred.append(y1_pred.cpu().numpy())\n",
    "            all_y0_true.append(batch['y0'].numpy())\n",
    "            all_y1_true.append(batch['y1'].numpy())\n",
    "            all_ite_true.append(batch['ite'].numpy())\n",
    "    \n",
    "    y0_pred = np.concatenate(all_y0_pred)\n",
    "    y1_pred = np.concatenate(all_y1_pred)\n",
    "    y0_true = np.concatenate(all_y0_true)\n",
    "    y1_true = np.concatenate(all_y1_true)\n",
    "    ite_true = np.concatenate(all_ite_true)\n",
    "    \n",
    "    ite_pred = y1_pred - y0_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    pehe = np.sqrt(np.mean((ite_pred - ite_true) ** 2))\n",
    "    ate_true = ite_true.mean()\n",
    "    ate_pred = ite_pred.mean()\n",
    "    ate_error = np.abs(ate_true - ate_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"√PEHE (lower is better):    {pehe:.4f}\")\n",
    "    print(f\"True ATE:                   {ate_true:.4f}\")\n",
    "    print(f\"Predicted ATE:              {ate_pred:.4f}\")\n",
    "    print(f\"ATE Error:                  {ate_error:.4f}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return ite_pred, ite_true, y0_pred, y1_pred, y0_true, y1_true\n",
    "\n",
    "# Evaluate on test set\n",
    "ite_pred, ite_true, y0_pred, y1_pred, y0_true, y1_true = evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
